{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from collections import Counter\n",
    "import re\n",
    "import zipfile\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "import textblob, string\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMModel, LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def import_data(file):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape:  (7241, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['agriculture', 'culture', 'energy', 'labour', 'law', 'rights']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../Data/category-data/all_cat.csv', encoding='latin', header=None)\n",
    "df.columns = ['headline', 'category']\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "print('df shape: ', df.shape)\n",
    "labels = df.category.unique()\n",
    "labels = list(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    if len(text.split()) < 4:\n",
    "        return np.nan\n",
    "    return text\n",
    "\n",
    "def make_table(filename, c1, x_axis, c2, y_axis='Accuracy'):\n",
    "    filename = './docs/' + filename + '.csv'\n",
    "    table = pd.DataFrame({\n",
    "            x_axis: c1,\n",
    "            y_axis: c2\n",
    "            })\n",
    "    table.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (5773,)\n",
      "valid_x shape: (1444,)\n"
     ]
    }
   ],
   "source": [
    "df['headline'] = df['headline'].apply(lambda text: clean_text(text))\n",
    "df = df.dropna()\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "X = df['headline']\n",
    "y = encoder.fit_transform(df.category)\n",
    "# y = pd.get_dummies(df['category'])\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = tts(X, y, test_size=0.2, random_state=0)\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('valid_x shape:', valid_x.shape)\n",
    "# encoder = LabelEncoder()\n",
    "# train_y_onehot = train_y.copy().as_matrix()\n",
    "# train_y = encoder.fit_transform(train_y.idxmax(axis=1))\n",
    "# valid_y_onehot = valid_y.copy().as_matrix()\n",
    "# valid_y = encoder.fit_transform(valid_y.idxmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "culture        2179\n",
       "energy         1244\n",
       "labour         1223\n",
       "rights          975\n",
       "law             892\n",
       "agriculture     704\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The What Founders On Cofounding Relationships Building Community And Radical Authenticity'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = df.headline.to_list()\n",
    "sentences[2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(sen) for sen in sentences], bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of words and n-grams for different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_counts(df, X, y, ngrams=1):\n",
    "    vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                                 stop_words=stop_words, ngram_range=(ngrams, ngrams))\n",
    "    categories = list(y.columns)    \n",
    "    for category in categories:\n",
    "        filename = ('category/'+category+'_{}_grams').format(ngrams)\n",
    "        print(category + '...')\n",
    "        cat_df = df.where(df[category] == 1).dropna()\n",
    "        cat_headline = vect.fit_transform(cat_df['headline'])\n",
    "        sum_words = cat_headline.sum(axis=0)\n",
    "        words_freq = [[word, sum_words[0, idx]] for word, idx in vect.vocabulary_.items()]\n",
    "        words_freq = pd.DataFrame(sorted(words_freq, key = lambda x: x[1], reverse=True))\n",
    "        make_table(filename, words_freq[0], ('{}_grams').format(ngrams),\n",
    "                   words_freq[1], 'Frequency')\n",
    "\n",
    "get_ngram_counts(df, X, y, ngrams=3)\n",
    "\n",
    "max_sent_len = 0\n",
    "for i in range(len(X)):\n",
    "    max_sent_len = max(len(X.at[i].split()), max_sent_len)\n",
    "\n",
    "print(\"Length of longest sentence: \", max_sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction for supervised models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count Vectorizer\n",
    "\"\"\"\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern='\\w{1,}',\n",
    "                             stop_words=stop_words, ngram_range=(1,2))\n",
    "count_vect.fit(train_x)\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TF-IDF Vectorizer\n",
    "\"\"\"\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                             ngram_range=(1,2))\n",
    "tfidf_vect.fit(train_x)\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LDA topic modelling` is done to understand and identify the underlying topics prevalent in the sets of sentences belonging to a certain category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LDA topic modelling\n",
    "\"\"\"\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=20, \n",
    "                                      learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, train, label, valid, is_neural_net=False):\n",
    "    start = time.time()\n",
    "    \n",
    "    if is_neural_net:\n",
    "        classifier.fit(train, label, epochs=1, batch_size=256)\n",
    "        predictions = classifier.predict(valid)\n",
    "        print(\"Time taken: %.2f seconds\" % (time.time()-start))\n",
    "        return predictions, accuracy_score(predictions, valid_y_onehot)\n",
    "    else:\n",
    "        classifier.fit(train, label)\n",
    "        predictions = classifier.predict(valid)\n",
    "        train_predictions = classifier.predict(train)\n",
    "        print(\"Time taken: %.2f seconds\" % (time.time()-start))\n",
    "        return predictions, [accuracy_score(predictions, valid_y), accuracy_score(train_predictions, label)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Count and TF-IDF features using supervised models\n",
    "\n",
    "\n",
    "Models used are:\n",
    "\n",
    "    1. Logistic Regression\n",
    "    2. Naive Bayes\n",
    "    3. Random Forest\n",
    "    4. Gradient Boosting\n",
    "    5. XGBoost\n",
    "    6. LGBoost\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = LogisticRegression()\n",
    "LR.fit(xtrain_count, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel Count accuracy (test, train): [{.3f}, {.3f}] 0.8074792243767313 0.9892603499047289\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " agriculture       0.93      0.75      0.83       145\n",
      "     culture       0.69      0.92      0.79       425\n",
      "      energy       0.91      0.77      0.83       252\n",
      "      labour       0.92      0.71      0.80       254\n",
      "         law       0.74      0.66      0.70       172\n",
      "      rights       0.91      0.92      0.92       196\n",
      "\n",
      "    accuracy                           0.81      1444\n",
      "   macro avg       0.85      0.79      0.81      1444\n",
      "weighted avg       0.83      0.81      0.81      1444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_count_pred = LR.predict(xvalid_count)\n",
    "print(\"LR, WordLevel Count accuracy (test, train): [{.3f}, {.3f}]\",\n",
    "      accuracy_score(lr_count_pred, valid_y),\n",
    "      accuracy_score(LR.predict(xtrain_count), train_y))\n",
    "print(classification_report(valid_y, lr_count_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.96 seconds\n",
      "LR, WordLevel TF-IDF accuracy (test, train): [{.3f}, {.3f}] [0.7693905817174516, 0.8796119868352676]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " agriculture       0.99      0.58      0.73       145\n",
      "     culture       0.61      0.97      0.75       425\n",
      "      energy       0.92      0.66      0.77       252\n",
      "      labour       0.91      0.68      0.78       254\n",
      "         law       0.84      0.51      0.64       172\n",
      "      rights       0.90      0.96      0.93       196\n",
      "\n",
      "    accuracy                           0.77      1444\n",
      "   macro avg       0.86      0.73      0.77      1444\n",
      "weighted avg       0.82      0.77      0.77      1444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_tfidf_pred, accuracy = train_model(LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"LR, WordLevel TF-IDF accuracy (test, train): [{.3f}, {.3f}]\", accuracy)      \n",
    "print(classification_report(valid_y, lr_tfidf_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.04 seconds\n",
      "Naive Bayes, Count Vectors accuracy (test, train):  [0.7991689750692521, 0.9819851030659968]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " agriculture       0.93      0.76      0.84       145\n",
      "     culture       0.76      0.84      0.80       425\n",
      "      energy       0.83      0.81      0.82       252\n",
      "      labour       0.82      0.75      0.78       254\n",
      "         law       0.73      0.65      0.69       172\n",
      "      rights       0.81      0.94      0.87       196\n",
      "\n",
      "    accuracy                           0.80      1444\n",
      "   macro avg       0.81      0.79      0.80      1444\n",
      "weighted avg       0.80      0.80      0.80      1444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_count_pred, accuracy = train_model(MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"Naive Bayes, Count Vectors accuracy (test, train): \", accuracy)\n",
    "print(classification_report(valid_y, nb_count_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.05 seconds\n",
      "Naive Bayes, WordLevel TF-IDF accuracy (test, train):  [0.6655124653739612, 0.8796119868352676]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " agriculture       1.00      0.28      0.43       145\n",
      "     culture       0.50      0.99      0.66       425\n",
      "      energy       0.93      0.64      0.76       252\n",
      "      labour       0.88      0.56      0.68       254\n",
      "         law       0.82      0.18      0.30       172\n",
      "      rights       0.89      0.85      0.87       196\n",
      "\n",
      "    accuracy                           0.67      1444\n",
      "   macro avg       0.83      0.58      0.62      1444\n",
      "weighted avg       0.78      0.67      0.64      1444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_tfidf_pred, accuracy = train_model(MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"Naive Bayes, WordLevel TF-IDF accuracy (test, train): \", accuracy)\n",
    "print(classification_report(valid_y, nb_tfidf_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 0.129\n",
      "SGD, Count Vectors accuracy (test, train):  0.8116343490304709\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " agriculture       0.90      0.77      0.83       145\n",
      "     culture       0.71      0.89      0.79       425\n",
      "      energy       0.89      0.79      0.83       252\n",
      "      labour       0.92      0.72      0.81       254\n",
      "         law       0.73      0.67      0.70       172\n",
      "      rights       0.90      0.94      0.92       196\n",
      "\n",
      "    accuracy                           0.81      1444\n",
      "   macro avg       0.84      0.80      0.81      1444\n",
      "weighted avg       0.83      0.81      0.81      1444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SGD = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "\n",
    "start = time.time()\n",
    "SGD.fit(xtrain_count, train_y)\n",
    "print('Time taken %.3f' % (time.time()-start))\n",
    "sgd_count_pred = SGD.predict(xvalid_count)\n",
    "print(\"SGD, Count Vectors accuracy (test, train): \", accuracy_score(sgd_count_pred, valid_y))\n",
    "print(classification_report(valid_y, sgd_count_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 25.824\n",
      "RandomForest, Count Vectors accuracy (test, train):  0.7915512465373962\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " agriculture       0.96      0.76      0.85       145\n",
      "     culture       0.67      0.89      0.76       425\n",
      "      energy       0.92      0.76      0.83       252\n",
      "      labour       0.92      0.69      0.79       254\n",
      "         law       0.68      0.65      0.67       172\n",
      "      rights       0.88      0.90      0.89       196\n",
      "\n",
      "    accuracy                           0.79      1444\n",
      "   macro avg       0.84      0.77      0.80      1444\n",
      "weighted avg       0.82      0.79      0.79      1444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "start = time.time()\n",
    "Forest.fit(xtrain_count, train_y)\n",
    "forest_count_pred = Forest.predict(xvalid_count)\n",
    "print('Time taken %.3f' % (time.time()-start))\n",
    "print(\"RandomForest, Count Vectors accuracy (test, train): \", accuracy_score(forest_count_pred, valid_y))\n",
    "print(classification_report(valid_y, forest_count_pred, target_names=labels))\n",
    "\n",
    "# forest_tfidf_pred, accuracy = train_model(Forest, xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "# print(\"RandomForest, TF-IDF accuracy (test, train): \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1        9005.8492            1.21m\n",
      "         2        8459.9861            1.21m\n",
      "         3        8056.5137            1.18m\n",
      "         4        7750.9876            1.18m\n",
      "         5        7491.0045            1.15m\n",
      "         6        7271.5394            1.14m\n",
      "         7        7088.1523            1.08m\n",
      "         8        6927.1777           58.93s\n",
      "         9        6784.2388           54.47s\n",
      "        10        6663.7182           50.85s\n",
      "        20        5873.3789           33.07s\n",
      "        30        5433.5912           32.70s\n",
      "        40        5101.6246           26.31s\n",
      "        50        4840.1554           20.58s\n",
      "        60        4631.0654           17.66s\n",
      "        70        4438.8051           12.42s\n",
      "        80        4276.2110            8.29s\n",
      "        90        4144.3979            4.21s\n",
      "       100        4007.2345            0.00s\n",
      "GBoost, Count Vectors accuracy (test, train):  0.7541551246537396\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " agriculture       0.96      0.72      0.83       145\n",
      "     culture       0.58      0.93      0.72       425\n",
      "      energy       0.95      0.62      0.75       252\n",
      "      labour       0.96      0.65      0.78       254\n",
      "         law       0.73      0.58      0.65       172\n",
      "      rights       0.90      0.86      0.88       196\n",
      "\n",
      "    accuracy                           0.75      1444\n",
      "   macro avg       0.85      0.73      0.77      1444\n",
      "weighted avg       0.81      0.75      0.76      1444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GBoost = GradientBoostingClassifier(n_estimators=30, learning_rate=0.05, \n",
    "#                                     max_depth=4, max_features='sqrt', \n",
    "#                                     min_samples_leaf=10, min_samples_split=10, \n",
    "#                                     random_state=5, verbose=1)\n",
    "GBoost = GradientBoostingClassifier(random_state=0, verbose=1)\n",
    "GBoost.fit(xtrain_count.tocsc(), train_y)\n",
    "gb_count_pred = GBoost.predict(xvalid_count.tocsc())\n",
    "print(\"GBoost, Count Vectors accuracy (test, train): \", accuracy_score(gb_count_pred, valid_y))\n",
    "print(classification_report(valid_y, gb_count_pred, target_names=labels))\n",
    "\n",
    "# gb_count_pred, accuracy = train_model(GBoost, xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "# print(\"GBoost, Count Vectors accuracy (test, train): \", accuracy)\n",
    "\n",
    "# gb_tfidf_pred, accuracy = train_model(GBoost, xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "# print(\"GBoost, WordLevel TF-IDF accuracy (test, train): \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB, Count Vectors accuracy (test, train):  0.6876731301939059\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " agriculture       0.93      0.56      0.70       145\n",
      "     culture       0.51      0.92      0.66       425\n",
      "      energy       0.96      0.54      0.69       252\n",
      "      labour       0.97      0.59      0.74       254\n",
      "         law       0.67      0.48      0.56       172\n",
      "      rights       0.89      0.77      0.82       196\n",
      "\n",
      "    accuracy                           0.69      1444\n",
      "   macro avg       0.82      0.64      0.69      1444\n",
      "weighted avg       0.78      0.69      0.69      1444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "XGBoost = XGBClassifier(colsample_bytree=0.8, gamma=0.0468,\n",
    "                             learning_rate=0.1, max_depth=5,\n",
    "                             min_child_weight=1, n_estimators=10,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.8, silent=1, verbose_eval=True,\n",
    "                             random_state=2019, n_thread=-1, verbose=1)\n",
    "\n",
    "XGBoost.fit(xtrain_count.tocsc(), train_y)\n",
    "xgb_count_pred = XGBoost.predict(xvalid_count.tocsc())\n",
    "print('Time taken %.3f' % (time.time()-start))\n",
    "print(\"XGB, Count Vectors accuracy (test, train): \", accuracy_score(xgb_count_pred, valid_y))\n",
    "print(classification_report(valid_y, xgb_count_pred, target_names=labels))\n",
    "\n",
    "# xgb_count_pred, accuracy = train_model(XGBoost, xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "# print(\"XGB, Count Vectors accuracy (test, train): \", accuracy)\n",
    "\n",
    "# xgb_tfidf_pred, accuracy = train_model(XGBoost, xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "# print(\"XGB, WordLevel TF-IDF accuracy (test, train): \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "category_recognition",
   "language": "python",
   "name": "category_recognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
