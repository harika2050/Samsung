{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Sat Aug 14 13:36:31 2019\n",
    "\n",
    "@author: Paras\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from collections import Counter\n",
    "import re\n",
    "import zipfile\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from typing import Dict, List, Sequence, Iterable\n",
    "import itertools\n",
    "import logging\n",
    "import math\n",
    "from keras.preprocessing import text, sequence\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, Input\n",
    "from keras.layers import Dense, Input, Dropout, Embedding, LSTM, SpatialDropout1D, Flatten\n",
    "from keras.layers import GRU, Bidirectional, Convolution1D, GlobalMaxPool1D, TimeDistributed, Lambda\n",
    "from keras.layers.merge import add\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report, accuracy_score\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from allennlp.common.checks import ConfigurationError\n",
    "# from allennlp.common.file_utils import cached_path\n",
    "# from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "# from allennlp.data.dataset_readers.dataset_utils import to_bioul\n",
    "# from allennlp.data.fields import TextField, SequenceLabelField, Field, MetadataField\n",
    "# from allennlp.data.instance import Instance\n",
    "# from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "# from allennlp.data.tokenizers import Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))    \n",
    "    return df\n",
    "\n",
    "def import_data(file, encoding='utf-8'):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    df = pd.read_csv(file, parse_dates=True, keep_date_col=True, encoding=encoding)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "def to_IOB(tag):\n",
    "    if 'U-' in tag:\n",
    "        tag = tag.replace('U-', 'B-')\n",
    "    elif 'L-' in tag:\n",
    "        tag = tag.replace('L-', 'I-')\n",
    "    \n",
    "    return tag\n",
    "\n",
    "def format_tags(tag):\n",
    "    entities = ['CARDINAL','TIME','DATE','ORDINAL','ORG','MONEY','PERCENT','LANGUAGE','LAW','QUANTITY']\n",
    "    tag = 'O' if any(entity in tag for entity in entities) else tag\n",
    "    return tag\n",
    "    \n",
    "def map_fire_tags(tag):\n",
    "    entities = ['PERSON','GPE','-LOC','-FAC','ORG','WORK_OF_ART','EVENT','NORP','PRODUCT']\n",
    "    tag = tag if any(entity in tag for entity in entities) else 'O'\n",
    "    tag = 'O' if 'LOCO' in tag else tag\n",
    "    return tag\n",
    "    \n",
    "def preprocess_fire(fire_df):\n",
    "    fire_df = fire_df.iloc[28:, [0,1,4]]\n",
    "    fire_df.columns = ['Sentence #', 'Word', 'Tag']\n",
    "    fire_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Assign sentence boundaries\n",
    "    prev = 1\n",
    "    for i in range(len(fire_df)):\n",
    "        if type(fire_df.at[i, 'Word']) == str and fire_df.at[i, 'Word'] == '*':\n",
    "            prev+=1\n",
    "        elif type(fire_df.at[i, 'Word']) == float and math.isnan(fire_df.at[i, 'Word']):\n",
    "            prev += 1\n",
    "        fire_df.at[i, 'Sentence #'] = prev\n",
    "        \n",
    "    # removal of excess tags\n",
    "    fire_df['Tag'] = fire_df['Tag'].apply(lambda tag: map_fire_tags(format_tags(tag)))\n",
    "    fire_df = fire_df.dropna()\n",
    "    return fire_df\n",
    "\n",
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            p_i = np.argmax(p)\n",
    "            out_i.append(idx2tag[p_i].replace(\"PADword\", \"O\"))\n",
    "        out.append(out_i)\n",
    "    return out\n",
    "\n",
    "def test2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            out_i.append(idx2tag[p].replace(\"PADword\", \"O\"))\n",
    "        out.append(out_i)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the OntoNotes NER Dataset that we have extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 259.55 MB\n",
      "Memory usage after optimization is: 52.97 MB\n",
      "Decreased by 79.6%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Like</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>many</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Heartland</td>\n",
       "      <td>U-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>states</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #       Word    Tag\n",
       "0           0       Like      O\n",
       "1           1       many      O\n",
       "2           2  Heartland  U-LOC\n",
       "3           3     states      O\n",
       "4           4          ,      O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = import_data('../../../Data/ner-data/ontonotes_raw.csv')\n",
    "df = df.iloc[:, [4,5,13]]\n",
    "df.columns = ['Sentence #','Word','Tag']\n",
    "df['Sentence #'] = df['Sentence #'].astype('int64')\n",
    "df.dropna(inplace=True)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "for col in df.columns[1:]:\n",
    "    df[col] = df[col].astype('object')\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'U-LOC', 'U-GPE', 'U-ORG', 'B-PERSON', 'L-PERSON', 'B-TIME',\n",
       "       'L-TIME', 'B-GPE', 'I-GPE', 'L-GPE', 'U-CARDINAL', 'B-LOC',\n",
       "       'L-LOC', 'U-NORP', 'B-NORP', 'L-NORP', 'B-DATE', 'I-DATE',\n",
       "       'L-DATE', 'U-ORDINAL', 'I-PERSON', 'B-QUANTITY', 'L-QUANTITY',\n",
       "       'B-ORG', 'L-ORG', 'U-DATE', 'B-MONEY', 'I-MONEY', 'L-MONEY',\n",
       "       'U-MONEY', 'U-PERSON', 'U-TIME', 'B-CARDINAL', 'I-CARDINAL',\n",
       "       'L-CARDINAL', 'I-ORG', 'U-FAC', 'I-LOC', 'U-PRODUCT', 'B-EVENT',\n",
       "       'L-EVENT', 'B-PRODUCT', 'I-PRODUCT', 'L-PRODUCT', 'B-FAC', 'I-FAC',\n",
       "       'L-FAC', 'I-QUANTITY', 'B-PERCENT', 'L-PERCENT', 'B-WORK_OF_ART',\n",
       "       'I-WORK_OF_ART', 'L-WORK_OF_ART', 'I-TIME', 'I-EVENT', 'I-PERCENT',\n",
       "       'U-WORK_OF_ART', 'U-QUANTITY', 'I-NORP', 'B-LAW', 'I-LAW', 'L-LAW',\n",
       "       'U-LAW', 'U-LANGUAGE', 'B-ORDINAL', 'L-ORDINAL', 'U-EVENT',\n",
       "       'B-LANGUAGE', 'I-LANGUAGE', 'L-LANGUAGE', 'I-ORDINAL', 'U-PERCENT'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Like</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>many</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Heartland</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>states</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #       Word    Tag\n",
       "0           0       Like      O\n",
       "1           1       many      O\n",
       "2           2  Heartland  B-LOC\n",
       "3           3     states      O\n",
       "4           4          ,      O"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onto_df.columns = ['Sentence #', 'Word', 'Tag']\n",
    "entities = ['CARDINAL','TIME','DATE','ORDINAL','MONEY','PERCENT','LANGUAGE','LAW','QUANTITY']\n",
    "onto_df['Tag'] = onto_df['Tag'].apply(lambda tag: to_IOB(tag))\n",
    "onto_df['Tag'] = onto_df['Tag'].apply(lambda tag: 'O' if any(entity in tag for entity in entities) else tag)\n",
    "onto_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>708724</td>\n",
       "      <td>13</td>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708725</td>\n",
       "      <td>14</td>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708726</td>\n",
       "      <td>15</td>\n",
       "      <td>man</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708727</td>\n",
       "      <td>16</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708728</td>\n",
       "      <td>17</td>\n",
       "      <td>God</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentence # Word Tag\n",
       "708724          13   is   O\n",
       "708725          14    a   O\n",
       "708726          15  man   O\n",
       "708727          16   of   O\n",
       "708728          17  God   O"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onto_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               644647\n",
       "GPE             14119\n",
       "PERSON          18933\n",
       "ORG             16999\n",
       "NORP             5210\n",
       "WORK_OF_ART      2364\n",
       "EVENT            1868\n",
       "LOC              2113\n",
       "FAC              1604\n",
       "PRODUCT           872\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = onto_df['Tag'].value_counts()\n",
    "all_tags = pd.Series()\n",
    "for val in values.index:\n",
    "    if val[2:] in all_tags:\n",
    "        all_tags[val[2:]] += values[val]\n",
    "    else:\n",
    "        all_tags[val[2:]] = values[val]\n",
    "all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = 1\n",
    "for i in range(len(onto_df)-1):\n",
    "    if onto_df.at[i, 'Sentence #'] > onto_df.at[i+1, 'Sentence #']:\n",
    "        onto_df.at[i, 'Sentence #'] = prev\n",
    "        prev += 1\n",
    "    else:\n",
    "        onto_df.at[i, 'Sentence #'] = prev\n",
    "        \n",
    "onto_df.at[len(onto_df)-1, 'Sentence #'] = prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in OntoNotes dataset: 42010\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences in OntoNotes dataset:', onto_df['Sentence #'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>708724</td>\n",
       "      <td>42010</td>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708725</td>\n",
       "      <td>42010</td>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708726</td>\n",
       "      <td>42010</td>\n",
       "      <td>man</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708727</td>\n",
       "      <td>42010</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708728</td>\n",
       "      <td>42010</td>\n",
       "      <td>God</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentence # Word Tag\n",
       "708724       42010   is   O\n",
       "708725       42010    a   O\n",
       "708726       42010  man   O\n",
       "708727       42010   of   O\n",
       "708728       42010  God   O"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onto_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     2,     3, ..., 42008, 42009, 42010])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onto_df['Sentence #'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the online NER dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_df = import_data('../../data/ner-data/ner_dataset.csv', encoding='latin1').drop('POS', axis=1)\n",
    "online_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_df['Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tags(tag):\n",
    "    if 'geo' in tag:\n",
    "        tag = tag.replace('geo', 'GPE')\n",
    "    elif 'gpe' in tag:\n",
    "        tag = tag.replace('gpe', 'LOC')\n",
    "    elif 'tim' in tag:\n",
    "        tag = 'O'\n",
    "    elif 'org' in tag:\n",
    "        tag = tag.replace('org', 'ORG')\n",
    "    elif 'per' in tag:\n",
    "        tag = tag.replace('per', 'PERSON')\n",
    "    elif 'art' in tag:\n",
    "        tag = tag.replace('art', 'WORK_OF_ART')\n",
    "    elif 'eve' in tag:\n",
    "        tag = tag.replace('eve', 'EVENT')\n",
    "    elif 'nat' in tag:\n",
    "        tag = tag.replace('nat', 'EVENT')\n",
    "        \n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_df['Tag'] = online_df['Tag'].apply(lambda x: map_tags(x))\n",
    "online_df['Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count of tags present in Online NER Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = online_df['Tag'].value_counts()\n",
    "all_tags = pd.Series()\n",
    "for val in values.index:\n",
    "    if val[2:] in all_tags:\n",
    "        all_tags[val[2:]] += values[val]\n",
    "    else:\n",
    "        all_tags[val[2:]] = values[val]\n",
    "all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_df['Sentence #'] = online_df['Sentence #'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_df['Sentence #'][1] == 'nan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = 42011\n",
    "for i in range(len(online_df)):\n",
    "    if online_df.at[i, 'Sentence #'] != 'nan':\n",
    "        prev += 1        \n",
    "    online_df.at[i, 'Sentence #'] = prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of sentences in Online NER:', online_df['Sentence #'].nunique())\n",
    "online_df['Sentence #'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter_online = SentenceGetter(online_df)\n",
    "sentences = getter_online.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sentences[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = [sentence for sentence in tmp for word in sentence if 'LOC' in word[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining both the datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Onto_df columns:', onto_df.columns)\n",
    "print('Online_df columns:', online_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([onto_df, online_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Combined df shape:', combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting data into format to feed to Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s['Word'].values.tolist(),s['Tag'].values.tolist())]\n",
    "        self.grouped = self.data.groupby('Sentence #').apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped.loc[self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df = import_data('../combined_ner_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Like', 'O'), ('many', 'O'), ('Heartland', 'B-LOC'), ('states', 'O'), (',', 'O'), ('Iowa', 'B-GPE'), ('has', 'O'), ('had', 'O'), ('trouble', 'O'), ('keeping', 'O'), ('young', 'O'), ('people', 'O'), ('down', 'O'), ('on', 'O'), ('the', 'O'), ('farm', 'O'), ('or', 'O'), ('anywhere', 'O'), ('within', 'O'), ('state', 'O'), ('lines', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# df = online_df.copy()\n",
    "df = onto_df.copy()\n",
    "# df = combined_df.copy()\n",
    "getter = SentenceGetter(df)\n",
    "sent = getter.get_next()\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42010\n"
     ]
    }
   ],
   "source": [
    "sentences = getter.sentences\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence has 210 words\n"
     ]
    }
   ],
   "source": [
    "largest_sen = max(len(sen) for sen in sentences)\n",
    "print('Longest sentence has {} words'.format(largest_sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAO9ElEQVR4nO3db4xcV33G8e/TmECBliTEilLb6rrFamUqFaJVSAVCKmnzF9WpRJGrqljIkt+YFiqk1ikvUvFHMlJLClKJ5GIXgxAhClSxGlTqhkhVXxCyJmnAcdMsJBBbTrLgEGhRAcOvL+YYhnTHO7bXM5s93480mnvPOXf23KO7z9w9c/dOqgpJUh9+btodkCRNjqEvSR0x9CWpI4a+JHXE0JekjqyZdgdO59JLL62ZmZlpd0OSnlcOHTr0zapau1jdig79mZkZ5ubmpt0NSXpeSfL1UXVO70hSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkdW9H/knquZXXcvWv747hsn3BNJWhk805ekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6shYoZ/kz5IcTvKVJJ9M8qIkG5Pcl2Q+yaeSXNjavrCtz7f6maHXubmVP5Lk2vOzS5KkUZYM/STrgD8FZqvqN4ALgK3A+4Fbq+oVwDPA9rbJduCZVn5ra0eSzW27VwLXAR9OcsHy7o4k6XTGnd5ZA/x8kjXAi4HjwBuAO1v9fuCmtrylrdPqr06SVn57VX2/qh4D5oErz30XJEnjWjL0q+oY8NfANxiE/bPAIeDbVXWyNTsKrGvL64An2rYnW/uXD5cvss1PJNmRZC7J3MLCwtnskyRphHGmdy5mcJa+Efgl4CUMpmfOi6raU1WzVTW7du3a8/VjJKlL40zv/A7wWFUtVNUPgc8ArwUuatM9AOuBY235GLABoNW/DPjWcPki20iSJmCc0P8GcFWSF7e5+auBh4F7gTe1NtuAu9rygbZOq/98VVUr39qu7tkIbAK+uDy7IUkax5qlGlTVfUnuBL4EnAQeAPYAdwO3J3lvK9vbNtkLfDzJPHCCwRU7VNXhJHcweMM4Ceysqh8t8/5Ikk5jydAHqKpbgFueU/w1Frn6pqr+F/iDEa/zPuB9Z9hHSdIy8T9yJakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHVkrNswrDYzu+5etPzx3TdOuCeSNFme6UtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUkbFCP8lFSe5M8p9JjiT5rSSXJDmY5NH2fHFrmyQfSjKf5KEkVwy9zrbW/tEk287XTkmSFjfumf4HgX+uql8HfhM4AuwC7qmqTcA9bR3gemBTe+wAbgNIcglwC/Aa4ErgllNvFJKkyVgy9JO8DHg9sBegqn5QVd8GtgD7W7P9wE1teQvwsRr4AnBRksuBa4GDVXWiqp4BDgLXLeveSJJOa5wz/Y3AAvAPSR5I8pEkLwEuq6rjrc2TwGVteR3wxND2R1vZqPKfkWRHkrkkcwsLC2e2N5Kk0xon9NcAVwC3VdWrgf/hp1M5AFRVAbUcHaqqPVU1W1Wza9euXY6XlCQ144T+UeBoVd3X1u9k8CbwVJu2oT0/3eqPARuGtl/fykaVS5ImZMnQr6ongSeS/Foruhp4GDgAnLoCZxtwV1s+ALylXcVzFfBsmwb6HHBNkovbB7jXtDJJ0oSsGbPdnwCfSHIh8DXgrQzeMO5Ish34OvDm1vazwA3APPC91paqOpHkPcD9rd27q+rEsuyFJGksY4V+VT0IzC5SdfUibQvYOeJ19gH7zqSDkqTl43/kSlJHxp3e6cLMrrsXLX98940T7okknR+e6UtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1ZM20O/B8MLPr7kXLH99944R7IknnxjN9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI2OHfpILkjyQ5J/a+sYk9yWZT/KpJBe28he29flWPzP0Gje38keSXLvcOyNJOr0zOdN/O3BkaP39wK1V9QrgGWB7K98OPNPKb23tSLIZ2Aq8ErgO+HCSC86t+5KkMzFW6CdZD9wIfKStB3gDcGdrsh+4qS1vaeu0+qtb+y3A7VX1/ap6DJgHrlyOnZAkjWfcM/2/Bf4c+HFbfznw7ao62daPAuva8jrgCYBW/2xr/5PyRbb5iSQ7kswlmVtYWDiDXZEkLWXJ0E/yRuDpqjo0gf5QVXuqaraqZteuXTuJHylJ3RjnS1ReC/xekhuAFwG/CHwQuCjJmnY2vx441tofAzYAR5OsAV4GfGuo/JThbSRJE7DkmX5V3VxV66tqhsEHsZ+vqj8C7gXe1JptA+5qywfaOq3+81VVrXxru7pnI7AJ+OKy7YkkaUnn8nWJfwHcnuS9wAPA3la+F/h4knngBIM3CqrqcJI7gIeBk8DOqvrROfx8SdIZyuAkfGWanZ2tubm5s95+1Hfbnm9+d66kaUpyqKpmF6vzP3IlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR05l2/O0gijvrzFL1eRNG2e6UtSRwx9SeqIoS9JHTH0JakjfpA7QX7AK2naPNOXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkeWDP0kG5Lcm+ThJIeTvL2VX5LkYJJH2/PFrTxJPpRkPslDSa4Yeq1trf2jSbadv92SJC1mnDP9k8A7q2ozcBWwM8lmYBdwT1VtAu5p6wDXA5vaYwdwGwzeJIBbgNcAVwK3nHqjkCRNxpKhX1XHq+pLbfm7wBFgHbAF2N+a7QduastbgI/VwBeAi5JcDlwLHKyqE1X1DHAQuG5Z90aSdFpnNKefZAZ4NXAfcFlVHW9VTwKXteV1wBNDmx1tZaPKn/szdiSZSzK3sLBwJt2TJC1h7NBP8lLg08A7quo7w3VVVUAtR4eqak9VzVbV7Nq1a5fjJSVJzVihn+QFDAL/E1X1mVb8VJu2oT0/3cqPARuGNl/fykaVS5ImZJyrdwLsBY5U1QeGqg4Ap67A2QbcNVT+lnYVz1XAs20a6HPANUkubh/gXtPKJEkTMs535L4W+GPgy0kebGV/CewG7kiyHfg68OZW91ngBmAe+B7wVoCqOpHkPcD9rd27q+rEsuyFJGksS4Z+Vf07kBHVVy/SvoCdI15rH7DvTDooSVo+/keuJHXE0Jekjhj6ktQRQ1+SOjLO1Ts6z2Z23b1o+eO7b5xwTyStdp7pS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIt2FYwbw9g6Tl5pm+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHvHrnecireiSdLc/0Jakjhr4kdcTQl6SOOKe/ioya6wfn+yUNeKYvSR0x9CWpI07vdMLLPCWBZ/qS1BVDX5I64vRO55z2kfrimb4kdcTQl6SOOL2jRTntI61OEw/9JNcBHwQuAD5SVbsn3QedPd8MpOe3iYZ+kguAvwN+FzgK3J/kQFU9PMl+aPmd7hYQi/FNQpqOSZ/pXwnMV9XXAJLcDmwBDP3OnOmbxHLxzUa9m3TorwOeGFo/CrxmuEGSHcCOtvrfSR45w59xKfDNs+5hH7odo7x/7KbdjtEZcIyWNq0x+uVRFSvug9yq2gPsOdvtk8xV1ewydmnVcYyW5hgtzTFa2koco0lfsnkM2DC0vr6VSZImYNKhfz+wKcnGJBcCW4EDE+6DJHVrotM7VXUyyduAzzG4ZHNfVR1e5h9z1lNDHXGMluYYLc0xWtqKG6NU1bT7IEmaEG/DIEkdMfQlqSOrJvSTXJfkkSTzSXZNuz8rRZLHk3w5yYNJ5lrZJUkOJnm0PV887X5OWpJ9SZ5O8pWhskXHJQMfasfWQ0mumF7PJ2fEGP1VkmPteHowyQ1DdTe3MXokybXT6fVkJdmQ5N4kDyc5nOTtrXzFHkurIvSHbu9wPbAZ+MMkm6fbqxXlt6vqVUPXC+8C7qmqTcA9bb03HwWue07ZqHG5HtjUHjuA2ybUx2n7KP9/jABubcfTq6rqswDt920r8Mq2zYfb7+VqdxJ4Z1VtBq4CdraxWLHH0qoIfYZu71BVPwBO3d5Bi9sC7G/L+4GbptiXqaiqfwNOPKd41LhsAT5WA18ALkpy+WR6Oj0jxmiULcDtVfX9qnoMmGfwe7mqVdXxqvpSW/4ucITBnQdW7LG0WkJ/sds7rJtSX1aaAv4lyaF2iwuAy6rqeFt+ErhsOl1bcUaNi8fXz3pbm5rYNzQ12P0YJZkBXg3cxwo+llZL6Gu011XVFQz+rNyZ5PXDlTW4Ztfrdp/DcRnpNuBXgVcBx4G/mW53VoYkLwU+Dbyjqr4zXLfSjqXVEvre3mGEqjrWnp8G/pHBn9xPnfqTsj0/Pb0eriijxsXjq6mqp6rqR1X1Y+Dv+ekUTrdjlOQFDAL/E1X1mVa8Yo+l1RL63t5hEUlekuQXTi0D1wBfYTA221qzbcBd0+nhijNqXA4Ab2lXXlwFPDv0p3tXnjP//PsMjicYjNHWJC9MspHBB5VfnHT/Ji1JgL3Akar6wFDVyj2WqmpVPIAbgP8Cvgq8a9r9WQkP4FeA/2iPw6fGBXg5gysKHgX+Fbhk2n2dwth8ksH0xA8ZzKtuHzUuQBhcHfZV4MvA7LT7P8Ux+ngbg4cYBNjlQ+3f1cboEeD6afd/QmP0OgZTNw8BD7bHDSv5WPI2DJLUkdUyvSNJGoOhL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjryfz1mzaW+7oGtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(sen) for sen in getter.sentences], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longest sentence has `210 words` in it and we can see that almost all of the sentences have less than `50 words` in them.\n",
    "Therefore, the sentences can all be padded or truncated to sequences of size `50` whilst adding a pad word to the smaller sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Both',\n",
       " 'sides',\n",
       " 'have',\n",
       " 'agreed',\n",
       " 'in',\n",
       " 'principle',\n",
       " 'to',\n",
       " 'Mr.',\n",
       " 'Clinton',\n",
       " \"'s\",\n",
       " 'framework',\n",
       " 'for',\n",
       " 'peace',\n",
       " ',',\n",
       " 'but',\n",
       " 'with',\n",
       " 'reservations',\n",
       " '.',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword',\n",
       " 'PADword']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 50\n",
    "X = [[w[0]for w in s] for s in sentences]\n",
    "new_X = []\n",
    "for seq in X:\n",
    "    new_seq = []\n",
    "    for i in range(max_len):\n",
    "        try:\n",
    "            new_seq.append(seq[i])\n",
    "        except:\n",
    "            new_seq.append(\"PADword\")\n",
    "    new_X.append(new_seq)\n",
    "new_X[2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags:  19\n",
      "['B-PRODUCT', 'B-EVENT', 'I-PERSON', 'B-GPE', 'O', 'B-ORG', 'I-PRODUCT', 'I-NORP', 'I-GPE', 'I-ORG', 'I-LOC', 'I-EVENT', 'I-WORK_OF_ART', 'B-NORP', 'B-PERSON', 'I-FAC', 'B-FAC', 'B-WORK_OF_ART', 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "tags = list(set(df['Tag'].values))\n",
    "n_tags = len(tags)\n",
    "print('Number of tags: ', n_tags)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Named entities` are treated as labels that are mapped to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags2indexfile = open('tags2index', 'rb')\n",
    "tags2index = pickle.load(tags2indexfile)\n",
    "\n",
    "idx2tagfile = open('idx2tag', 'rb')\n",
    "idx2tag = pickle.load(idx2tagfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-FAC': 1,\n",
       " 'B-WORK_OF_ART': 2,\n",
       " 'B-ORG': 3,\n",
       " 'I-NORP': 4,\n",
       " 'I-LOC': 5,\n",
       " 'B-PRODUCT': 6,\n",
       " 'B-PERSON': 7,\n",
       " 'I-GPE': 8,\n",
       " 'I-PRODUCT': 9,\n",
       " 'B-LOC': 10,\n",
       " 'B-GPE': 11,\n",
       " 'I-PERSON': 12,\n",
       " 'B-EVENT': 13,\n",
       " 'I-FAC': 14,\n",
       " 'I-WORK_OF_ART': 15,\n",
       " 'I-ORG': 16,\n",
       " 'I-EVENT': 17,\n",
       " 'B-NORP': 18}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-FAC',\n",
       " 2: 'B-WORK_OF_ART',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-NORP',\n",
       " 5: 'I-LOC',\n",
       " 6: 'B-PRODUCT',\n",
       " 7: 'B-PERSON',\n",
       " 8: 'I-GPE',\n",
       " 9: 'I-PRODUCT',\n",
       " 10: 'B-LOC',\n",
       " 11: 'B-GPE',\n",
       " 12: 'I-PERSON',\n",
       " 13: 'B-EVENT',\n",
       " 14: 'I-FAC',\n",
       " 15: 'I-WORK_OF_ART',\n",
       " 16: 'I-ORG',\n",
       " 17: 'I-EVENT',\n",
       " 18: 'B-NORP'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags2index = {t:i for i,t in enumerate(tags)}\n",
    "\n",
    "# idx2tag = {i: w for w, i in tags2index.items()}\n",
    "\n",
    "y = [[tags2index[w[1]] for w in s] for s in sentences]\n",
    "y = sequence.pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tags2index[\"O\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0 10  0  0 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags2indexfile = open('tags2index', 'wb')\n",
    "# pickle.dump(tags2index, tags2indexfile)\n",
    "\n",
    "# idx2tagfile = open('idx2tag', 'wb')\n",
    "# pickle.dump(idx2tag, idx2tagfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data and loading the `ELMo embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = tts(new_X, y, test_size=0.2, random_state=0)\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (33608, 50)\n",
      "y_train shape: (33608, 50)\n",
      "x_test shape: (8402, 50)\n",
      "y_test shape: (8402, 50)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', pd.DataFrame(x_train).shape)\n",
    "print('y_train shape:', pd.DataFrame(y_train).shape)\n",
    "print('x_test shape:', pd.DataFrame(x_test).shape)\n",
    "print('y_test shape:', pd.DataFrame(y_test).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use a function to convert sentences to `ELMo embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "def ElmoEmbedding(x):\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_hub as hub\n",
    "    batch_size = 32\n",
    "    max_len = 50\n",
    "    elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "    return elmo_model(inputs={\"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                              \"sequence_len\": tf.constant(batch_size*[max_len])},\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "        self.val_accuracy = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.model.validation_data[0]))).round()\n",
    "        val_targ = self.model.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        _val_recall = recall_score(val_targ, val_predict)\n",
    "        _val_precision = precision_score(val_targ, val_predict)\n",
    "        _val_accuracy = accuracy_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        self.val_accuracy.append(_val_accuracy)\n",
    "        print(\"— val_f1: %f — val_precision: %f — val_recall %f\" %(_val_f1, _val_precision, _val_recall, _val_accuracy))\n",
    "        return\n",
    "\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0906 16:33:12.704216 139897425884928 deprecation_wrapper.py:119] From /home/shreyas_n1799/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0906 16:33:12.706939 139897425884928 deprecation_wrapper.py:119] From /home/shreyas_n1799/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0906 16:33:15.683041 139897425884928 deprecation_wrapper.py:119] From /home/shreyas_n1799/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0906 16:33:19.249548 139897425884928 deprecation_wrapper.py:119] From /home/shreyas_n1799/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0906 16:33:19.264463 139897425884928 deprecation.py:506] From /home/shreyas_n1799/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0906 16:33:24.369240 139897425884928 deprecation_wrapper.py:119] From /home/shreyas_n1799/venv/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0906 16:33:24.413560 139897425884928 deprecation_wrapper.py:119] From /home/shreyas_n1799/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = Input(shape=(max_len,), dtype='string')\n",
    "embedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(input_text)\n",
    "x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                       recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
    "x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                           recurrent_dropout=0.2, dropout=0.2))(x)\n",
    "x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\n",
    "model = Model(input_text, out)\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for onto_df\n",
    "a = 850\n",
    "b = 200\n",
    "c = 262\n",
    "\n",
    "# # for online_df\n",
    "# a = 1100\n",
    "# b = 248\n",
    "\n",
    "# for combined_df\n",
    "# a = 2249\n",
    "# b = 562"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid = x_train[:a*batch_size], x_train[-b*batch_size:]\n",
    "y_train, y_valid = y_train[:a*batch_size], y_train[-b*batch_size:]\n",
    "y_train = y_train.reshape(y_train.shape[0], y_train.shape[1], 1)\n",
    "y_valid = y_valid.reshape(y_valid.shape[0], y_valid.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train shape: (27200, 50, 1)\n",
      "y valid shape: (6400, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "print('y train shape:', y_train.shape)\n",
    "print('y valid shape:', y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"new-onto-model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 4192 samples\n",
      "Epoch 1/4\n",
      "33600/33600 [==============================] - 3487s 104ms/step - loss: 0.0315 - acc: 0.9920 - val_loss: 0.0240 - val_acc: 0.9941\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.03147, saving model to new-onto-model.h5\n",
      "Epoch 2/4\n",
      "33600/33600 [==============================] - 3579s 107ms/step - loss: 0.0219 - acc: 0.9942 - val_loss: 0.0214 - val_acc: 0.9945\n",
      "\n",
      "Epoch 00002: loss improved from 0.03147 to 0.02193, saving model to new-onto-model.h5\n",
      "Epoch 3/4\n",
      "33600/33600 [==============================] - 3583s 107ms/step - loss: 0.0174 - acc: 0.9951 - val_loss: 0.0215 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00003: loss improved from 0.02193 to 0.01743, saving model to new-onto-model.h5\n",
      "Epoch 4/4\n",
      "33600/33600 [==============================] - 3575s 106ms/step - loss: 0.0137 - acc: 0.9960 - val_loss: 0.0228 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00004: loss improved from 0.01743 to 0.01365, saving model to new-onto-model.h5\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(x_train), y_train,\n",
    "                    validation_data=(np.array(x_valid), y_valid), \n",
    "                    batch_size=batch_size, epochs=4, verbose=1, \n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop and continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model(\"new-onto-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "new_model.fit(x_train, y_train, epochs=5, batch_size=50, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('new-onto-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0906 16:34:29.229415 139897425884928 deprecation.py:323] From /home/shreyas_n1799/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model_name = 'new-onto-model.h5'\n",
    "model = load_model(model_name)\n",
    "print('{} has been loaded'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8402, 50)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8384/8384 [==============================] - 598s 71ms/step\n"
     ]
    }
   ],
   "source": [
    "x_test = x_test[:c*batch_size]\n",
    "test_pred = model.predict(np.array(x_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8402, 50)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     PERSON       0.95      0.95      0.95      2200\n",
      "      EVENT       0.74      0.73      0.74       112\n",
      "        ORG       0.86      0.84      0.85      1294\n",
      "       NORP       0.95      0.89      0.92       975\n",
      "        GPE       0.93      0.89      0.91      2326\n",
      "        FAC       0.78      0.68      0.73       103\n",
      "        LOC       0.76      0.64      0.69       217\n",
      "WORK_OF_ART       0.61      0.58      0.60       148\n",
      "    PRODUCT       0.80      0.84      0.82        83\n",
      "\n",
      "  micro avg       0.91      0.88      0.89      7458\n",
      "  macro avg       0.91      0.88      0.89      7458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels = pred2label(test_pred)\n",
    "test_labels = test2label(y_test[:c*batch_size])\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on the FIRE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0829 19:39:45.709926 140018061465344 deprecation_wrapper.py:119] From /home/shreyas_n1799/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0829 19:39:45.715103 140018061465344 deprecation_wrapper.py:119] From /home/shreyas_n1799/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0829 19:39:47.475060 140018061465344 deprecation_wrapper.py:119] From /home/shreyas_n1799/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0829 19:39:48.411564 140018061465344 deprecation_wrapper.py:119] From /home/shreyas_n1799/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0829 19:39:48.419430 140018061465344 deprecation.py:506] From /home/shreyas_n1799/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0829 19:39:50.656605 140018061465344 deprecation_wrapper.py:119] From /home/shreyas_n1799/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0829 19:39:52.011184 140018061465344 deprecation_wrapper.py:119] From /home/shreyas_n1799/venv/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0829 19:39:52.496736 140018061465344 deprecation.py:323] From /home/shreyas_n1799/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "onto_model = load_model('onto-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 5.49 MB\n",
      "Memory usage after optimization is: 1.85 MB\n",
      "Decreased by 66.3%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>URL</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>;</td>\n",
       "      <td>:</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>http</td>\n",
       "      <td>NN</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>/</td>\n",
       "      <td>SYM</td>\n",
       "      <td>B-NP</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     0    1     2  3  4  5  6\n",
       "0           0   URL  NNP  B-NP  o  o  o  o\n",
       "1           1     ;    :     o  o  o  o  o\n",
       "2           2  http   NN  B-NP  o  o  o  o\n",
       "3           3     :    :     o  o  o  o  o\n",
       "4           4     /  SYM  B-NP  o  o  o  o"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_df = import_data('../../../data/ner-data/fire_ner.csv')\n",
    "fire_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Scheme</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>specially</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>designed</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Government</td>\n",
       "      <td>B-NORP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #        Word     Tag\n",
       "0           1      Scheme       O\n",
       "1           1   specially       O\n",
       "2           1    designed       O\n",
       "3           1         for       O\n",
       "4           1  Government  B-NORP"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_df = preprocess_fire(fire_df)\n",
    "fire_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                79806\n",
       "B-PERSON          1854\n",
       "I-PERSON          1465\n",
       "B-GPE             1229\n",
       "B-NORP             325\n",
       "I-GPE              314\n",
       "B-LOC              291\n",
       "I-EVENT            279\n",
       "B-EVENT            237\n",
       "I-NORP             199\n",
       "I-LOC              142\n",
       "I-WORK_OF_ART      133\n",
       "I-FAC              123\n",
       "B-WORK_OF_ART      112\n",
       "B-FAC               69\n",
       "B-PRODUCT           29\n",
       "I-PRODUCT            2\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_df['Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fire_to_test(fire_df):\n",
    "    \n",
    "    # For the sentences\n",
    "    max_len = 50\n",
    "    getter_fire = SentenceGetter(fire_df)\n",
    "    sentences = getter_fire.sentences\n",
    "    X = [[w[0]for w in s] for s in sentences]\n",
    "    new_X = []\n",
    "    for seq in X:\n",
    "        new_seq = []\n",
    "        for i in range(max_len):\n",
    "            try:\n",
    "                new_seq.append(seq[i])\n",
    "            except:\n",
    "                new_seq.append(\"PADword\")\n",
    "        new_X.append(new_seq)\n",
    "        \n",
    "    # For the tags    \n",
    "    y_fire = [[tags2index[w[1]] for w in s] for s in sentences]\n",
    "    y_fire = sequence.pad_sequences(maxlen=max_len, sequences=y_fire, padding=\"post\", value=tags2index[\"O\"])\n",
    "    return new_X, y_fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fire_x shape: 3515\n",
      "fire_y shape: 3515\n"
     ]
    }
   ],
   "source": [
    "fire_x, fire_y = fire_to_test(fire_df)\n",
    "print('fire_x shape:', len(fire_x))\n",
    "print('fire_y shape:', len(fire_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 113s 71ms/step\n"
     ]
    }
   ],
   "source": [
    "fire_x = fire_x[:50*batch_size]\n",
    "fire_pred = onto_model.predict(np.array(fire_x), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        GPE       0.73      0.62      0.67       555\n",
      "      EVENT       0.14      0.14      0.14       186\n",
      "        LOC       0.09      0.07      0.08        88\n",
      "     PERSON       0.74      0.55      0.63       773\n",
      "       NORP       0.13      0.20      0.16       115\n",
      "WORK_OF_ART       0.06      0.13      0.08        39\n",
      "        FAC       0.04      0.10      0.05        20\n",
      "    PRODUCT       0.00      0.00      0.00         5\n",
      "\n",
      "  micro avg       0.37      0.47      0.42      1781\n",
      "  macro avg       0.58      0.47      0.51      1781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_pred_labels = pred2label(fire_pred)\n",
    "fire_test_labels = test2label(fire_y[:50*batch_size])\n",
    "# print(classification_report(fire_pred_labels, fire_test_labels))\n",
    "print(classification_report(fire_test_labels, fire_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
